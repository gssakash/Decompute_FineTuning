{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gssakash/Decompute_FineTuning/blob/main/Decompute.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This finetuning notebook uses Unsloth AI [https://unsloth.ai/] which proves to be extremely helpful for the purpose of LLM Finetuning.\n",
        "\n",
        "It also offers a speed boost of 1.6x as compared to FastAttention and more benefits.\n",
        "\n",
        "References :\n",
        "\n",
        "1. https://blog.monsterapi.ai/blogs/unsloth-sdpa-integrated-in-monsterapi/#:~:text=Unsloth%20is%20a%20method%20designed,token%20handling%20and%20parallel%20processing.\n",
        "\n",
        "2. https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article"
      ],
      "metadata": {
        "id": "lLw3OO3EZBjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth \"xformers==0.0.28.post2\"\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "EsapV-6-IDyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQzRMyiplSde",
        "outputId": "9f197fba-fa9d-484c-94ec-cb40535e02a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.0+cpu\n",
            "Uninstalling torch-2.5.0+cpu:\n",
            "  Successfully uninstalled torch-2.5.0+cpu\n",
            "Found existing installation: torchvision 0.20.0+cpu\n",
            "Uninstalling torchvision-0.20.0+cpu:\n",
            "  Successfully uninstalled torchvision-0.20.0+cpu\n",
            "Found existing installation: torchaudio 2.5.0+cpu\n",
            "Uninstalling torchaudio-2.5.0+cpu:\n",
            "  Successfully uninstalled torchaudio-2.5.0+cpu\n",
            "Collecting torch\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xformers 0.0.28.post2 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow && pip install tensorflow-cpu"
      ],
      "metadata": {
        "id": "vP-nWa1rJx3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8Sj7dpElFOy",
        "outputId": "2ff98113-a799-43bf-ca93-1a8608f22ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Set maximum sequence length\n",
        "max_seq_length = 5020\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"unsloth/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "from peft import get_peft_config, LoraConfig, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 5020\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state = 32,\n",
        "    loftq_config = None,\n",
        ")\n",
        "print(model.print_trainable_parameters())"
      ],
      "metadata": {
        "id": "IZSMUCnQY0xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5MpO4OKpetb"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXz73TJNpXDM"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3FY7915mNpb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "# Load datasets\n",
        "boolq_dataset = load_dataset(\"google/boolq\", trust_remote_code=True)\n",
        "piqa_dataset = load_dataset(\"ybisk/piqa\", split='train', trust_remote_code=True)\n",
        "winogrande_dataset = load_dataset(\"allenai/winogrande\", 'winogrande_xs', trust_remote_code=True)\n",
        "\n",
        "def format_boolq(example):\n",
        "    \"\"\"\n",
        "    Format a single example from the BoolQ dataset into a dictionary.\n",
        "    \"\"\"\n",
        "    question = example[\"question\"]  # Extract question\n",
        "    answer = \"Yes\" if example[\"answer\"] else \"No\"  # Convert boolean to Yes/No\n",
        "\n",
        "    return {\n",
        "        \"text\": f\"Question: {question}\\nAnswer: {answer}\"  # Return as text\n",
        "    }\n",
        "\n",
        "def format_piqa(example):\n",
        "    \"\"\"\n",
        "    Format a single example from the PIQA dataset into a structured string.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"text\": f\"Goal: {example['goal']}\\nSolution 1: {example['sol1']}\\nSolution 2: {example['sol2']}\"  # Return as text\n",
        "    }\n",
        "\n",
        "def format_winogrande(example):\n",
        "    \"\"\"\n",
        "    Format a single example from the Winogrande dataset into a structured string.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"text\": f\"Sentence: {example['sentence']}\\nOption 1: {example['option1']}\\nOption 2: {example['option2']}\"  # Return as text\n",
        "    }\n",
        "\n",
        "# Format each dataset\n",
        "boolq_formatted = boolq_dataset['train'].map(format_boolq)\n",
        "piqa_formatted = piqa_dataset.map(format_piqa)\n",
        "winogrande_formatted = winogrande_dataset['train'].map(format_winogrande)\n",
        "\n",
        "# Ensure all datasets have consistent features before concatenation\n",
        "boolq_formatted = boolq_formatted.remove_columns([col for col in boolq_formatted.column_names if col not in [\"formatted_question\", \"formatted_answer\"]])\n",
        "piqa_formatted = piqa_formatted.remove_columns([col for col in piqa_formatted.column_names if col not in [\"goal\", \"sol1\", \"sol2\"]])\n",
        "winogrande_formatted = winogrande_formatted.remove_columns([col for col in winogrande_formatted.column_names if col not in [\"sentence\", \"option1\", \"option2\"]])\n",
        "\n",
        "# Split the PIQA dataset into train, validation, and test sets\n",
        "piqa_train_test_split = piqa_formatted.train_test_split(test_size=0.2)  # Split into train and test\n",
        "piqa_train_val_split = piqa_train_test_split['train'].train_test_split(test_size=0.25)  # Split train into train and validation\n",
        "\n",
        "# Combine the datasets\n",
        "combined_dataset = concatenate_datasets([\n",
        "    boolq_formatted,\n",
        "    piqa_train_val_split['train'],\n",
        "    piqa_train_val_split['test'],\n",
        "    winogrande_formatted\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdVmc3hlmP8P"
      },
      "outputs": [],
      "source": [
        "# Tokenize the combined dataset with efficient memory usage\n",
        "def tokenize_function(examples):\n",
        "    # Combine the goal and solutions into a single text format\n",
        "    texts = [f\"Goal: {g}\\nSolution 1: {s1}\\nSolution 2: {s2}\"\n",
        "             for g, s1, s2 in zip(examples['goal'], examples['sol1'], examples['sol2'])]\n",
        "\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
        "\n",
        "# Tokenize the combined datasets\n",
        "tokenized_combined_datasets = combined_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Tokenize the combined datasets\n",
        "tokenized_combined_datasets = combined_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1Quui9NmSPA"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "\n",
        "# Split the combined dataset into training and evaluation sets\n",
        "train_test_split = tokenized_combined_datasets.train_test_split(test_size=0.1)  # 10% for evaluation\n",
        "\n",
        "# Set up training arguments with mixed precision and gradient accumulation to reduce memory footprint\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    per_device_train_batch_size=8,  # Adjusted batch size for memory constraints\n",
        "    num_train_epochs=40,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=1,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradients over multiple steps\n",
        ")\n",
        "\n",
        "# Initialize the SFTTrainer with both train and eval datasets\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_test_split['train'],\n",
        "    eval_dataset=train_test_split['test'],  # Add evaluation dataset here\n",
        "    dataset_text_field=\"text\",  # Specify the text field in your dataset\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,  # Number of processes for loading datasets\n",
        "    packing=True,  # Enable packing\n",
        "    args=training_args,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start training on the combined dataset while monitoring metrics\n",
        "trainer.train()\n",
        "\n",
        "# Clear CUDA cache after training or when needed\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "BsroaoOyY_VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmxih_hMmUnj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def infer(question):\n",
        "    input_text = f\"Question: {question}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate response from model using no_grad to save memory during inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example inference\n",
        "question_example = \"Is Python a programming language?\"\n",
        "print(infer(question_example))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}